{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyOfL9FKDoG5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlcq1HAs7meI"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USiZZJaUFQcT"
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  df.head()\n",
    "  Y = df['5'].to_numpy()\n",
    "  del df['5']\n",
    "  X=df.to_numpy()\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVXIEJCvD0dJ"
   },
   "outputs": [],
   "source": [
    "X, y = load_data(\"/home/anushk/Downloads/mnist_train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                X, y, test_size=0.3, random_state=42)\n",
    "#One hot encoding of training labels \n",
    "Labels=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zyoHKGD7tvJ"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aYMsXGyDwqJ"
   },
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            x=self.softmax(z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            prev_term=self.delta_mll(y,self.y_pred)  \n",
    "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            x=self.softmax(z) \n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "TjgzI7Ss7dbD",
    "outputId": "fe2eff6f-534a-4c6a-ba87-f4a4e6f664c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.001586018585227259\n",
      "1 0.0023587135652579022\n",
      "2 0.002003204942718389\n",
      "3 0.0016388985292124934\n",
      "4 0.0013583852913124144\n",
      "5 0.001153716608271859\n",
      "6 0.0008646485922930012\n",
      "7 0.0007199554874074769\n",
      "8 0.0005575510844699823\n",
      "9 0.0004922387907553779\n",
      "10 0.0004415899489354481\n",
      "11 0.0003315042154667358\n",
      "12 0.00022569601738133493\n",
      "13 0.00019177360006609185\n",
      "14 0.00015197409696228914\n",
      "15 0.00012457834892370357\n",
      "16 0.00011646801061476971\n",
      "17 0.00011791686444778907\n",
      "18 0.00012066810857163411\n",
      "19 0.000119697301402828\n",
      "20 0.00011583933624036273\n",
      "21 0.00010640359842480589\n",
      "22 9.819335539868544e-05\n",
      "23 9.463012824271954e-05\n",
      "24 9.282139362919509e-05\n",
      "25 9.162837462707141e-05\n",
      "26 9.040493293463654e-05\n",
      "27 8.93485256543466e-05\n",
      "28 8.957033520565239e-05\n",
      "29 9.092390003069985e-05\n"
     ]
    }
   ],
   "source": [
    "n=Perceptron(X_train,Labels)\n",
    "n.connect(X_train,Labels)\n",
    "n.train(batches=1000,lr=0.2,epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "OhDzZkLr-Bg0",
    "outputId": "58dea9f6-b86b-4369-c100-1f91f5007ddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([313, 332, 289, 350, 331, 274, 287, 313, 229, 282]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z2jK6jKC-Il3",
    "outputId": "be985372-bf87-4242-d9c2-5d2e78094c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 88.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klZhm_WFDeV-"
   },
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYE-TondDhAH"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class SingleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "wffR59W-D9zR",
    "outputId": "8c31cd9c-7a48-4e1e-b839-4c37c4f88479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00047046164962920463\n",
      "1 0.0003164448908505148\n",
      "2 0.00029779591371988475\n",
      "3 0.00030931882572054375\n",
      "4 0.00029596854351050095\n",
      "5 0.00027778394763262063\n",
      "6 0.0002447842042903175\n",
      "7 0.00020392309168323574\n",
      "8 0.00017089251196913298\n",
      "9 0.00014741776841326892\n",
      "10 0.00012978147076433258\n",
      "11 0.00011382983030810124\n",
      "12 9.94421062783665e-05\n",
      "13 8.61942745197484e-05\n",
      "14 7.418776440610189e-05\n",
      "15 6.487556655064246e-05\n",
      "16 5.858262525230289e-05\n",
      "17 5.377430837631162e-05\n",
      "18 4.9569670079672724e-05\n",
      "19 4.5814798885718326e-05\n",
      "20 4.2417384219600686e-05\n",
      "21 3.934391725110332e-05\n",
      "22 3.659930750152086e-05\n",
      "23 3.415938349994702e-05\n",
      "24 3.1973331059461936e-05\n",
      "25 3.000452110534015e-05\n",
      "26 2.8247042090109395e-05\n",
      "27 2.6714473830488164e-05\n",
      "28 2.5405508207495572e-05\n",
      "29 2.428761805848322e-05\n",
      "30 2.3314914462669763e-05\n",
      "31 2.244672582338746e-05\n",
      "32 2.1652902023901806e-05\n",
      "33 2.0913184520556367e-05\n",
      "34 2.02151537731737e-05\n",
      "35 1.9551671518693655e-05\n",
      "36 1.8918615868274223e-05\n",
      "37 1.831339068589022e-05\n",
      "38 1.773411257418594e-05\n",
      "39 1.717920128959215e-05\n",
      "40 1.66471863149143e-05\n",
      "41 1.613663841004861e-05\n",
      "42 1.5646174536947736e-05\n",
      "43 1.5174495941653908e-05\n",
      "44 1.4720429044663747e-05\n",
      "45 1.428295206896048e-05\n",
      "46 1.3861202439749905e-05\n",
      "47 1.3454467538710367e-05\n",
      "48 1.306216438418606e-05\n",
      "49 1.2683813878494464e-05\n"
     ]
    }
   ],
   "source": [
    "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dp0shsvSEWvN",
    "outputId": "5039d921-26a6-43ba-e8a4-eba8489e2e7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([308, 327, 303, 322, 319, 270, 287, 331, 245, 288]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "knkPcfW8EYx5",
    "outputId": "c319fd28-79f5-4a04-9aec-152f3acd2ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 91.7 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AcxS5MRXF1km"
   },
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_nmBd8mF4d5"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class DoubleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "xJY6j6AiGCI3",
    "outputId": "cde1989d-7336-40e9-b2ad-a6ec109102ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0008694888668898743\n",
      "1 0.0005404375580100259\n",
      "2 0.0001980628493386742\n",
      "3 0.00015945575874796075\n",
      "4 0.00014878138361425203\n",
      "5 0.0001274566010924463\n",
      "6 0.00011849689193143795\n",
      "7 0.00010881895903108596\n",
      "8 9.999963308116773e-05\n",
      "9 9.315560532019121e-05\n",
      "10 8.799807612598008e-05\n",
      "11 8.07054422068905e-05\n",
      "12 7.1650232568252e-05\n",
      "13 6.16651739034767e-05\n",
      "14 5.2403122297094394e-05\n",
      "15 4.5201924353847266e-05\n",
      "16 3.995786141057449e-05\n",
      "17 3.6071396595296284e-05\n",
      "18 3.286877909209678e-05\n",
      "19 2.996245148817723e-05\n"
     ]
    }
   ],
   "source": [
    "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "l2=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mRAWf6C5GJgN",
    "outputId": "25d9b540-8076-4504-9130-dd1c5638ee66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([309, 326, 304, 342, 307, 262, 286, 339, 238, 287]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xmpGuKUbGMJ1",
    "outputId": "2ab4b7e5-9e2d-4eae-c400-cba966b2f011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 90.63333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCuNTnWEGOub"
   },
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-KrbHw9GlsV"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkActivations():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "JjlQBHZCHFWA",
    "outputId": "2be080cb-ee39-4697-b167-ed17e0f9e538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0011118531795551155\n",
      "1 0.0010161806025322949\n",
      "2 0.0001563820323457743\n",
      "3 0.00033936389499510935\n",
      "4 0.0007443295608376171\n",
      "5 0.0012609036525834407\n",
      "6 0.00020761804757190027\n",
      "7 0.00019690750648624356\n",
      "8 2.478924769462182e-05\n",
      "9 7.00696031552472e-05\n",
      "10 3.0479391848620226e-05\n",
      "11 0.0002791096316711005\n",
      "12 6.547871209239849e-06\n",
      "13 3.3051776806884943e-06\n",
      "14 7.5991684915775166e-06\n",
      "15 0.001056216897028776\n",
      "16 2.151698280701257e-06\n",
      "17 1.09850266197807e-05\n",
      "18 9.935262002407722e-06\n",
      "19 1.509381731392964e-05\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkActivations(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qZfek1XaL2PR",
    "outputId": "ff1d0cea-00ad-407d-eaa0-8e11f8c4b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([309, 325, 294, 316, 328, 275, 276, 331, 270, 276]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sd2UmE1jNEUV",
    "outputId": "eb10fff3-62d2-4332-ca7b-0e9658d65120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 92.5 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFVOwq1RQLVo"
   },
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyyRnpPaQRSS"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkMomentum():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        self.delta_weights=[]\n",
    "        self.delta_bias=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr,momentum=False,beta=0.9):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            if momentum:\n",
    "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
    "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
    "            else:\n",
    "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10,beta):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr,momentum=True,beta)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "5KieS6HyQuwx",
    "outputId": "72ae514a-6211-4948-db17-cd5092f16b4e"
   },
   "outputs": [],
   "source": [
    "n=NeuralNetworkMomentum(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=500,lr=0.1,epoch=20,beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "M6I2UDHGQy7c",
    "outputId": "32569bcd-dc09-4d76-8c74-adb0949961cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([299, 324, 310, 335, 301, 253, 292, 334, 266, 286]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "apHXOBwRQ60d",
    "outputId": "bbad07c7-96a6-4ad9-dda1-627f6cf30b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 92.83333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Assignment9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
